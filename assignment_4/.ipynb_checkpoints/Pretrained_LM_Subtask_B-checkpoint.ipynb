{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LP02Bz-_Depe"
   },
   "source": [
    "# Pre-trained Language Models: SubTask B\n",
    "## [6 Marks]\n",
    "\n",
    "In this assignment, you will work on the [ComVE](https://competitions.codalab.org/competitions/21080) shared task that was part of SemEval-2020. The task aims to evaluate whether a system can distinguish if a natural language statement makes sense to humans or not and provide a reason. **ConVE** includes three subtasks that require models to acquire and apply commonsense knowledge. In this notebook you will focus on **SubTask B**:\n",
    "\n",
    "- Given a statement that does not make sense and three possible reasons, select which reason explains why the given statement is against common sense. For example, for the following nonsensical statement the correct answer is *Reason A*:\n",
    "\n",
    "     *Statement*: He put an elephant into the fridge.  \n",
    "     *Reason A*: An elephant is much bigger than a fridge.  \n",
    "     *Reason B*: Elephants are usually white while fridges are usually white.  \n",
    "     *Reason C*: An elephant cannot eat a fridge.\n",
    "     \n",
    "     This subtask can be approached as a Multiple Choice problem where the input is the nonsensical statement and the three possible explanations, and the output is a label indicating which of the reasons is the correct one.\n",
    "\n",
    "You will fine-tune a Pre-trained Language Model with [Transformers](https://huggingface.co/docs/transformers/index) library that provides a set of tools for fine-tunning and deploying a wide variety of Pre-trained Language Models. The [Hugging Face Hub](https://huggingface.co/models) allows you to explore all the models supported by **Transformers** and even share your own models with the community. In this assignment, you will work with [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta), a model that uses **BERT**'s architecture but has been pre-trained with more data and a more carefully selected set of hyperparameters.\n",
    "\n",
    "Fine-tuning a Pre-trained Language Model usually requires a great amount of time and computational resources. Your personal computer will not be probably enough. In order to complete the assignment, you can work with a reduced version of the dataset and the base version of **RoBERTa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HP2NcFKuDepf"
   },
   "outputs": [],
   "source": [
    "shrink_dataset = True\n",
    "base_model = True\n",
    "colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mpqil617Depg"
   },
   "source": [
    "Although the value of these variables do not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` and a `base_model` variables set as `True`, and a `colab` variable set as `False`.\n",
    "\n",
    "If you want to perform a full training of the model to obtain its real performance, you can use a cloud service like [Google Colab](https://colab.research.google.com/). **Colab** is a **Jupyter** notebook environment that supports both GPU and TPU instances, allowing training large scale Deep Learning models. Set the `shrink_dataset` and a `base_model` variables to `False`, the `colab` variable to `True`, and follow the instructions provided to you to run the notebook in **Colab**.\n",
    "\n",
    "> **Note!** To run this notebook in **Colab** you will need to upload the `datacollator.py` file included in the repository of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7rj2PIOXDepg"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    ! pip install transformers datasets evaluate\n",
    "    import os\n",
    "    if not os.path.exists(\"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskA_data_all.csv\"):\n",
    "        ! git clone https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.git SemEval2020-Task4-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mMYU-eXXDepg"
   },
   "source": [
    "You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3NsmE_FwDepg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForMultipleChoice, \n",
    "                          TrainingArguments, Trainer, \n",
    "                          enable_full_determinism)\n",
    "from datacollator import DataCollatorForMultipleChoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JioQWyAcDepg"
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In Transformers, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f7XhKiC7Deph"
   },
   "outputs": [],
   "source": [
    "enable_full_determinism(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "L35uDVTdDeph"
   },
   "source": [
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "C_ok2o8FDeph"
   },
   "outputs": [],
   "source": [
    "epochs = 3  # Number of epochs to train the model\n",
    "train_batch_size = 8  # Number of examples used per gradient update\n",
    "learning_rate = 1e-5  # The learning rate for the optimizer\n",
    "max_length = 50  # Maximum lenght of the input sequence\n",
    "output_dir = \"modelB\"  # The output directory where the model will be written to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Lk32PWxzDeph"
   },
   "source": [
    "## Loading the Pre-trained Model - [1 Mark]\n",
    "\n",
    "The first step you must perform in this assignment is to load the model and its corresponding tokenizer. **Transformers** provides support for a wide variety of pre-trained models via specific classes. However, the library also allows automatically retrieving a model given jut the name or path using [AutoClasses](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto). To fine-tune a pre-trained model for a downstream task, it is necessary to replace the original top layer of the model with a new specific output layer. **AutoClasses** also allows you to do this automatically for various types of Natural Language Processing tasks. For instance, `AutoModelForMultipleChoice` instantiates the model with a top layer for Multiple Choice.\n",
    "\n",
    "You must complete the code for the `load_model` function. This functions takes the name of the pre-trained model and should load and return both the model, initialized for Text Classification, and its corresponding tokenizer. You can get some tips from [Transformers documentation](https://huggingface.co/docs/transformers/autoclass_tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JCoWDkMvDeph"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9Nu5FT6gDeph"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhay/Desktop/college/uoa/applied_nlp/assignments/assignment_4/env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultipleChoice: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\" if base_model else \"roberta-large\"\n",
    "model, tokenizer = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hxTrMZqtDeph"
   },
   "source": [
    "## Data Pre-processing - [1 Mark]\n",
    "\n",
    "The **ComVE** dataset consists of 9997 nonsensical statements with their corresponding 3 possible reasons for the train set, 997 statements for development and 1000 for test. Each nonsensical statements is annotated with a `A`, `B` or `C` label depending on which is the correct reason. The dataset can be loaded into three `DataFrames` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qT9Qr6OeDeph"
   },
   "outputs": [],
   "source": [
    "def load_data(data_csv, answers_csv, labels):\n",
    "    data = pd.read_csv(data_csv).dropna()\n",
    "    answers = pd.read_csv(answers_csv, header=None).rename(columns={0: \"id\", 1: \"label\"})\n",
    "    answers[\"label\"] = answers[\"label\"].apply(lambda x: labels.index(x))\n",
    "    return pd.merge(data, answers, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2IKy-Ii7Deph"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FalseSent</th>\n",
       "      <th>OptionA</th>\n",
       "      <th>OptionB</th>\n",
       "      <th>OptionC</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>4122</td>\n",
       "      <td>You are likely to find a computer in the bathroom</td>\n",
       "      <td>The computer needs to take a shower in the bat...</td>\n",
       "      <td>The computer may be broken in the bathroom</td>\n",
       "      <td>The computer won't walk into the bathroom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4065</th>\n",
       "      <td>4065</td>\n",
       "      <td>Something you find in a stone is a blue flower</td>\n",
       "      <td>Sometimes stones are heavier than flowers</td>\n",
       "      <td>Sometimes stones are lighter than flowers</td>\n",
       "      <td>Flowers cannot grow on stones</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1731</td>\n",
       "      <td>People use electricity to buy things</td>\n",
       "      <td>It is impossible to buy things with electricity</td>\n",
       "      <td>Electricity is essential to live</td>\n",
       "      <td>Many appliances in home works on electricity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>4740</td>\n",
       "      <td>There is a way to cure every kind of cancer now</td>\n",
       "      <td>Cancer can kill people in a very short time</td>\n",
       "      <td>There is not a way to cure every kind of cance...</td>\n",
       "      <td>There is currently no vaccine to prevent peopl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>6392</td>\n",
       "      <td>You can break the cement column</td>\n",
       "      <td>Cement column is so hard</td>\n",
       "      <td>Cement column is gray</td>\n",
       "      <td>Cement column has many shapes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>6836</td>\n",
       "      <td>Tall is a disease</td>\n",
       "      <td>Tall is a normal physiological phenomenon</td>\n",
       "      <td>Tall is determined by both environment and gene</td>\n",
       "      <td>Tall is an external expression</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>5399</td>\n",
       "      <td>Tara spread some gasoline on her bread</td>\n",
       "      <td>the price of gasoline has been raised rapidly</td>\n",
       "      <td>people have jam with bread</td>\n",
       "      <td>people do not have bread with gasoline</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>6183</td>\n",
       "      <td>he went to the school to deposit a sum of money</td>\n",
       "      <td>students take money to their school to pay the...</td>\n",
       "      <td>building a school costs a lot of money</td>\n",
       "      <td>the school has no place to save money</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>7538</td>\n",
       "      <td>You can only use chopsticks to eat noodles</td>\n",
       "      <td>You can also use forks to eat noodles</td>\n",
       "      <td>Many Chinese people use chopsticks to eat noodles</td>\n",
       "      <td>There are different types of noodles in the world</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6768</th>\n",
       "      <td>6769</td>\n",
       "      <td>Running is a disease</td>\n",
       "      <td>Running is a form of exercise.</td>\n",
       "      <td>Running is an aerobic exercise</td>\n",
       "      <td>Running can be aerobic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          FalseSent  \\\n",
       "4122  4122  You are likely to find a computer in the bathroom   \n",
       "4065  4065     Something you find in a stone is a blue flower   \n",
       "1731  1731               People use electricity to buy things   \n",
       "4740  4740    There is a way to cure every kind of cancer now   \n",
       "6391  6392                    You can break the cement column   \n",
       "...    ...                                                ...   \n",
       "6835  6836                                  Tall is a disease   \n",
       "5399  5399             Tara spread some gasoline on her bread   \n",
       "6182  6183    he went to the school to deposit a sum of money   \n",
       "7536  7538         You can only use chopsticks to eat noodles   \n",
       "6768  6769                               Running is a disease   \n",
       "\n",
       "                                                OptionA  \\\n",
       "4122  The computer needs to take a shower in the bat...   \n",
       "4065          Sometimes stones are heavier than flowers   \n",
       "1731    It is impossible to buy things with electricity   \n",
       "4740        Cancer can kill people in a very short time   \n",
       "6391                           Cement column is so hard   \n",
       "...                                                 ...   \n",
       "6835          Tall is a normal physiological phenomenon   \n",
       "5399      the price of gasoline has been raised rapidly   \n",
       "6182  students take money to their school to pay the...   \n",
       "7536              You can also use forks to eat noodles   \n",
       "6768                     Running is a form of exercise.   \n",
       "\n",
       "                                                OptionB  \\\n",
       "4122         The computer may be broken in the bathroom   \n",
       "4065          Sometimes stones are lighter than flowers   \n",
       "1731                   Electricity is essential to live   \n",
       "4740  There is not a way to cure every kind of cance...   \n",
       "6391                              Cement column is gray   \n",
       "...                                                 ...   \n",
       "6835    Tall is determined by both environment and gene   \n",
       "5399                         people have jam with bread   \n",
       "6182             building a school costs a lot of money   \n",
       "7536  Many Chinese people use chopsticks to eat noodles   \n",
       "6768                     Running is an aerobic exercise   \n",
       "\n",
       "                                                OptionC  label  \n",
       "4122          The computer won't walk into the bathroom      1  \n",
       "4065                      Flowers cannot grow on stones      2  \n",
       "1731       Many appliances in home works on electricity      0  \n",
       "4740  There is currently no vaccine to prevent peopl...      1  \n",
       "6391                      Cement column has many shapes      0  \n",
       "...                                                 ...    ...  \n",
       "6835                     Tall is an external expression      0  \n",
       "5399             people do not have bread with gasoline      2  \n",
       "6182              the school has no place to save money      2  \n",
       "7536  There are different types of noodles in the world      0  \n",
       "6768                             Running can be aerobic      0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"A\", \"B\", \"C\"]\n",
    "train_data_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskB_data_all.csv\"\n",
    "train_answers_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskB_answers_all.csv\"\n",
    "train_data = load_data(train_data_csv, train_answers_csv, labels)\n",
    "dev_data_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskB_dev_data.csv\"\n",
    "dev_answers_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskB_gold_answers.csv\"\n",
    "dev_data = load_data(dev_data_csv, dev_answers_csv, labels)\n",
    "test_data_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskB_test_data.csv\"\n",
    "test_answers_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskB_gold_answers.csv\"\n",
    "test_data = load_data(test_data_csv, test_answers_csv, labels)\n",
    "if shrink_dataset:\n",
    "    train_data = train_data.sample(n=100, random_state=42)\n",
    "    dev_data = dev_data.sample(n=100, random_state=42)\n",
    "    test_data = test_data.sample(n=100, random_state=42)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pYIs9L9QDeph"
   },
   "source": [
    "Notice that the `load_data` function translates the labels into their corresponding numerical index: `0`, `1` and `2`.\n",
    "\n",
    "[Datasets](https://huggingface.co/docs/datasets/index) is a library for dataset management that provides a set of tools to manipulate data in a easy and efficient way. Since it is fully integrated with **Transformers**, it is very convenient to use both libraries together. **Datasets** allows accessing and sharing datasets through the [Hugging Face Hub](https://huggingface.co/datasets). The core component of this library is the [Dataset](https://huggingface.co/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset) class that implements an [Apache Arrow table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html). Similar to a **pandas** `DataFrame`, a `Dataset` object stores a table where each row corresponds to an example of the dataset and each column contains a different type of data. There are different ways to load the data into a `Dataset`, for example, from a `Dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jTrZNaIYDeph"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4122,\n",
       " 'FalseSent': 'You are likely to find a computer in the bathroom',\n",
       " 'OptionA': 'The computer needs to take a shower in the bathroom',\n",
       " 'OptionB': 'The computer may be broken in the bathroom',\n",
       " 'OptionC': \"The computer won't walk into the bathroom\",\n",
       " 'label': 1,\n",
       " '__index_level_0__': 4122}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mhWiFNw0Deph"
   },
   "source": [
    "One of the most powerful **Datasets** tools is the [map](https://huggingface.co/docs/datasets/v2.10.0/en/nlp_process#map) function which allows pre-processing the dataset in batches. The function takes another callable as argument and applies it to every row in the `Dataset`. The goal of the next exercise is to implement a function to tokenize the statement pairs that will be used as a parameter of the `map` function.\n",
    "\n",
    "You must complete the code for the `preprocess_data` function. This function takes a batch of examples from a `Dataset`, the tokenizer returned by `load_model` and the `max_length` hyperparameter. The function should make three copies of each statement in the `FalseSent` field and pair them with each of the possible reasons in `OptionA`, `OptionB` and `OptionC`. Then, the statement-reason pairs must be tokenized jointly. The tokenizer must pad and truncate the sequences to the `max_length` value. You can use the [Preprocessing](https://huggingface.co/docs/transformers/v4.27.2/en/preprocessing) and the [Tokenizer](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/tokenizer) documentation as reference. \n",
    "\n",
    "The `tokenizer` should return a [BatchEncoding](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/tokenizer#transformers.BatchEncoding) object with two fields for each data example:\n",
    "* *input_ids*: A list of token indices that will be used as the input of the model.\n",
    "* *attention_mask*: A list of indices masking out which tokens the model should not attend to.\n",
    "\n",
    "After running the tokenizer, `preprocess_data` should unflatten the `input_ids` and `attention_mask` corresponding to the same statement, i.e., for each example, the value of `input_ids`  should be a list of three lists of token indices and, similarly, the value of `attention_mask` should be a list of three lists of masking indices. The **Transformers** documentation provides a [guide for Multiple Choice](https://huggingface.co/docs/transformers/tasks/multiple_choice) problems that you can use as reference.  The `preprocess_data` should return the output of the unflattening step.\n",
    "\n",
    "The `map` function takes the `input_ids` and `attention_mask` fields and inserts them into the `Dataset` as new two columns. For example, the result for the first row in the `Dataset` should look like:\n",
    "\n",
    "> <pre>\n",
    "{'id': 4122, 'FalseSent': 'You are likely to find a computer in the bathroom', 'OptionA': 'The computer needs to take a shower in the bathroom', 'OptionB': 'The computer may be broken in the bathroom', 'OptionC': \"The computer won't walk into the bathroom\", 'label': 1, '__index_level_0__': 4122, 'input_ids': [[0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 782, 7, 185, 10, 9310, 11, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 189, 28, 3187, 11, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 351, 75, 1656, 88, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
    "</pre>\n",
    "\n",
    "The `input_ids` field contains three lists, one for each statement-reason pair. Each value in each list of `input_ids` represents a sub-word of the `tokenizer` vocabulary. For the example above, `input_ids` corresponds to the following thee sequences of sub-words:\n",
    "\n",
    "> <pre>\n",
    "> ['&lt;s&gt;', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;/s&gt;', 'The', 'Ġcomputer', 'Ġneeds', 'Ġto', 'Ġtake', 'Ġa', 'Ġshower', 'Ġin', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n",
    "> \n",
    "> ['&lt;s&gt;', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;/s&gt;', 'The', 'Ġcomputer', 'Ġmay', 'Ġbe', 'Ġbroken', 'Ġin', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n",
    ">\n",
    "> ['&lt;s&gt;', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;/s&gt;', 'The', 'Ġcomputer', 'Ġwon', \"'t\", 'Ġwalk', 'Ġinto', 'Ġthe', 'Ġbathroom', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n",
    "</pre>\n",
    "\n",
    "\n",
    "Notice that the **Hugging Face** implementation of **RoBERTa**'s tokenizer uses the `<s>` token equivalently to **BERT**'s `[CLS]` token and the `</s>` token to mark both the end and the separation of the sentences. The `Ġ` character indicates when there is a blank space before the token in the original text. This helps to know which tokens are the first sub-words of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tLj_vD4IDepi"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, max_length):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "\n",
    "    # Making three copies of each statement in examples['FalseSent']\n",
    "    first_sentences = [[context] * 3 for context in examples[\"FalseSent\"]]\n",
    "    # Picking all options corresponding to the respective context\n",
    "    second_sentences = [\n",
    "        \n",
    "        [examples['OptionA'][i], examples['OptionB'][i], examples['OptionC'][i]] for i in range(len(examples['FalseSent']))               \n",
    "                       \n",
    "    ]\n",
    "\n",
    "    # Flattening both the lists\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Applying tokenizer\n",
    "    tokenizer_output = tokenizer(first_sentences, second_sentences, \n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   max_length=max_length, return_tensors='pt'\n",
    "                                  )\n",
    "    # Unflattening and returning the output\n",
    "    return {key: [values[i : i + 3] for i in range(0, len(values), 3)] for key, values in tokenizer_output.items()}\n",
    "\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zyLvZ14hDepi"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4122, 'FalseSent': 'You are likely to find a computer in the bathroom', 'OptionA': 'The computer needs to take a shower in the bathroom', 'OptionB': 'The computer may be broken in the bathroom', 'OptionC': \"The computer won't walk into the bathroom\", 'label': 1, '__index_level_0__': 4122, 'input_ids': [[0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 782, 7, 185, 10, 9310, 11, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 189, 28, 3187, 11, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1185, 32, 533, 7, 465, 10, 3034, 11, 5, 8080, 2, 2, 133, 3034, 351, 75, 1656, 88, 5, 8080, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "\n",
      "['<s>', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '</s>', '</s>', 'The', 'Ġcomputer', 'Ġneeds', 'Ġto', 'Ġtake', 'Ġa', 'Ġshower', 'Ġin', 'Ġthe', 'Ġbathroom', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "['<s>', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '</s>', '</s>', 'The', 'Ġcomputer', 'Ġmay', 'Ġbe', 'Ġbroken', 'Ġin', 'Ġthe', 'Ġbathroom', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "['<s>', 'You', 'Ġare', 'Ġlikely', 'Ġto', 'Ġfind', 'Ġa', 'Ġcomputer', 'Ġin', 'Ġthe', 'Ġbathroom', '</s>', '</s>', 'The', 'Ġcomputer', 'Ġwon', \"'t\", 'Ġwalk', 'Ġinto', 'Ġthe', 'Ġbathroom', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "dev_dataset = dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "print(train_dataset[0])\n",
    "print(\"\")\n",
    "for seq in train_dataset[0][\"input_ids\"]:\n",
    "    print(tokenizer.convert_ids_to_tokens(seq))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7SfBoXCvDepi"
   },
   "source": [
    "## Fine-tuning - [4 Marks]\n",
    "\n",
    "Although it is possible to write customized training loops for the **Transormers** models using **keras** or **pytorch**, **Transformers** provides a [Trainer](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer) API that allows fine-tuning efficiently with a few simple steps. The training is highly customizable through with a wide range of options and hyperparameters that are handled by the [TrainingArguments](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.TrainingArguments) class. Your next goal is to create both the `TrainingArguments` and `Trainer` objects that will be used to fine-tune **RoBERTa**. See the [documentation](https://huggingface.co/docs/transformers/training) for an introduction on how to perform these steps.\n",
    "\n",
    "You must complete the code for the `create_training_arguments` function. This function takes as arguments the `epochs`, `train_batch_size` and `learning_rate` hyperparameters along with the `output_dir`. The function should use these arguments to create and return a `TrainingArguments` object. During the training, the model must be evaluated on the development test after every epoch. `TrainingArguments` should include this strategy.\n",
    "\n",
    "> **Important!** By default, `Trainer` saves a checkpoint of the model every 500 training steps. For this assignment, avoid this behavior by setting `save_strategy=\"no\"` when creating the `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CewTLliNDepi"
   },
   "outputs": [],
   "source": [
    "def create_training_arguments(epochs, train_batch_size, learning_rate, output_dir):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=epochs, per_device_train_batch_size=train_batch_size,\n",
    "        learning_rate=learning_rate, output_dir=output_dir,\n",
    "        save_strategy='no', evaluation_strategy='epoch'\n",
    "    )\n",
    "    return training_args\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ASflHPEJDepi"
   },
   "outputs": [],
   "source": [
    "train_args = create_training_arguments(epochs, train_batch_size, learning_rate, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5EMcJZeGDepi"
   },
   "source": [
    "Next, you will create a `Trainer` object with the training arguments. When the input format of a task has some special characteristic, the `Trainer` must be created with a data collator that can handle the batches of examples accordingly during the training. This is the case of Multiple Choice problems since the input of each example is a list of sequences. **Transformers** provides a set of [DataCollator](https://huggingface.co/docs/transformers/main_classes/data_collator) objects for different tasks, but not for Multiple Choice. However, a `DataCollatorForMultipleChoice` is provided along with this notebook. \n",
    "\n",
    "You must complete the code for the `create_trainer` function. The function takes as input the model returned by `load_model`, the `TrainingArguments` created by `create_training_arguments` and the train and development `Datasets`. The function also takes the `tokenizer` returned by `load_model` that is required to initialize `DataCollatorForMultipleChoice`. The `create_trainer` function must create and return a `Trainer` object with the model, the training arguments and a `DataCollatorForMultipleChoice` object. The `Trainer` must be set up so that the train `Dataset` is used for training and the development `Dataset` is used to evaluate the model during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zccyIfJSDepi"
   },
   "outputs": [],
   "source": [
    "def create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    trainer_object = Trainer(\n",
    "        model=model, args=train_args,\n",
    "        train_dataset=train_dataset, eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer, data_collator=DataCollatorForMultipleChoice(tokenizer)\n",
    "    )\n",
    "    return trainer_object\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6FpTrleJDepi"
   },
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Mf8-ClKXDepi"
   },
   "source": [
    "The `trainer` object created by `create_trainer` is ready to fine-tune the model by just running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mutlJP5SDepi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__. If OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__ are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "/Users/abhay/Desktop/college/uoa/applied_nlp/assignments/assignment_4/env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.097995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.097187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.096923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__. If OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__ are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__. If OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__ are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__. If OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__ are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.1002799792167468, metrics={'train_runtime': 57.3816, 'train_samples_per_second': 5.228, 'train_steps_per_second': 0.68, 'total_flos': 23124787470000.0, 'train_loss': 1.1002799792167468, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iAPVZxvCDepi"
   },
   "source": [
    "After training, the model can be used to make predictions on unlabeled data using the [predict](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainer.predict) method of the `Trainer` class.\n",
    "\n",
    "You must complete the code for the `make_predictions` function. The function takes as input the `Trainer` object and test `Dataset`. The function must run the `predict` method on the input data. The `predict` method will return a `NamedTuple` including a **numpy** array with the predictions. For each statement in the input, the array contains a vector with the logits (the values used as input of the softmax) predicted for every label corresponding to a possible reason. The output of `make_predictions` must include only the index of the label with the highest logit value. For example, if the prediction for one statement is `[-0.856213458, 1.39899943, -0.703246286e]`, the output for that example should be `1`. For this, you can apply the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) method along the last axis of the **numpy** array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s_gKydP-Depi"
   },
   "outputs": [],
   "source": [
    "def make_predictions(trainer, test_dataset):   # [2 Marks]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    test_predictions = trainer.predict(test_dataset=test_dataset)\n",
    "    test_pred = test_predictions.predictions\n",
    "    predicted_label = []\n",
    "    for pred in test_pred:\n",
    "        predicted_label.append(pred.argmax())\n",
    "    return predicted_label\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "33MPmZvrDepj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__. If OptionC, FalseSent, id, OptionA, OptionB, __index_level_0__ are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FalseSent</th>\n",
       "      <th>OptionA</th>\n",
       "      <th>OptionB</th>\n",
       "      <th>OptionC</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>324</td>\n",
       "      <td>She put the filing cabinet into the papers.</td>\n",
       "      <td>Papers are fragile than the filing cabinet.</td>\n",
       "      <td>Nothing can be put into the paper.</td>\n",
       "      <td>Filing cabinets are usually gray while papers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1455</td>\n",
       "      <td>The lion used the litter box</td>\n",
       "      <td>A lion is normally found in the wild</td>\n",
       "      <td>A lion cannot eat a cat</td>\n",
       "      <td>A domestic cat is tame and use litter boxes</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>13</td>\n",
       "      <td>Cigarette is good for healthy</td>\n",
       "      <td>Cigarette contains lots of nicotines</td>\n",
       "      <td>Lung will be damaged by smoking cigarette</td>\n",
       "      <td>Cigarettes always have a high tax</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>207</td>\n",
       "      <td>Pens are for painting</td>\n",
       "      <td>Pens are too small</td>\n",
       "      <td>Pens are a writing utensil</td>\n",
       "      <td>pens would make a mess</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>774</td>\n",
       "      <td>he put a piece of plastic on the bread</td>\n",
       "      <td>I don't like sliced bread in plastic bags</td>\n",
       "      <td>the plastic usually is toxic</td>\n",
       "      <td>some plastic is biodegradable</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>225</td>\n",
       "      <td>Carol turned on the potato</td>\n",
       "      <td>Carol eats the potato</td>\n",
       "      <td>A potato can't be turned on</td>\n",
       "      <td>potatoes can be used to make electricity</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1663</td>\n",
       "      <td>i use my dog to play cricket</td>\n",
       "      <td>Dog is not interested in the cricket ball</td>\n",
       "      <td>the dog is so brisk</td>\n",
       "      <td>No one can play the cricket ball by their's dog</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>82</td>\n",
       "      <td>Dolphins are fish.</td>\n",
       "      <td>Dolphins are warm-blooded and breathe air, whi...</td>\n",
       "      <td>Dolphins have fins.</td>\n",
       "      <td>Dolphins live in the ocean.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>800</td>\n",
       "      <td>the family adopted a dinosaur to be their new pet</td>\n",
       "      <td>the dinosaurs died out long ago</td>\n",
       "      <td>many different animals can make good pets</td>\n",
       "      <td>some dinosaurs are carnivorous animals</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>948</td>\n",
       "      <td>he learned with his refrigerator before the exam</td>\n",
       "      <td>refrigerators are usually big while the exam i...</td>\n",
       "      <td>refrigerator doesn't have any information for ...</td>\n",
       "      <td>his mate hide his textbooks in the refrigerato...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          FalseSent  \\\n",
       "521   324        She put the filing cabinet into the papers.   \n",
       "737  1455                       The lion used the litter box   \n",
       "740    13                      Cigarette is good for healthy   \n",
       "660   207                              Pens are for painting   \n",
       "411   774             he put a piece of plastic on the bread   \n",
       "..    ...                                                ...   \n",
       "436   225                         Carol turned on the potato   \n",
       "764  1663                       i use my dog to play cricket   \n",
       "88     82                                 Dolphins are fish.   \n",
       "63    800  the family adopted a dinosaur to be their new pet   \n",
       "826   948   he learned with his refrigerator before the exam   \n",
       "\n",
       "                                               OptionA  \\\n",
       "521        Papers are fragile than the filing cabinet.   \n",
       "737               A lion is normally found in the wild   \n",
       "740               Cigarette contains lots of nicotines   \n",
       "660                                 Pens are too small   \n",
       "411          I don't like sliced bread in plastic bags   \n",
       "..                                                 ...   \n",
       "436                              Carol eats the potato   \n",
       "764          Dog is not interested in the cricket ball   \n",
       "88   Dolphins are warm-blooded and breathe air, whi...   \n",
       "63                     the dinosaurs died out long ago   \n",
       "826  refrigerators are usually big while the exam i...   \n",
       "\n",
       "                                               OptionB  \\\n",
       "521                 Nothing can be put into the paper.   \n",
       "737                            A lion cannot eat a cat   \n",
       "740          Lung will be damaged by smoking cigarette   \n",
       "660                         Pens are a writing utensil   \n",
       "411                       the plastic usually is toxic   \n",
       "..                                                 ...   \n",
       "436                        A potato can't be turned on   \n",
       "764                                the dog is so brisk   \n",
       "88                                 Dolphins have fins.   \n",
       "63           many different animals can make good pets   \n",
       "826  refrigerator doesn't have any information for ...   \n",
       "\n",
       "                                               OptionC  label  prediction  \n",
       "521  Filing cabinets are usually gray while papers ...      1           0  \n",
       "737        A domestic cat is tame and use litter boxes      2           0  \n",
       "740                  Cigarettes always have a high tax      1           0  \n",
       "660                             pens would make a mess      1           1  \n",
       "411                      some plastic is biodegradable      1           0  \n",
       "..                                                 ...    ...         ...  \n",
       "436           potatoes can be used to make electricity      1           1  \n",
       "764    No one can play the cricket ball by their's dog      2           0  \n",
       "88                         Dolphins live in the ocean.      0           0  \n",
       "63              some dinosaurs are carnivorous animals      0           0  \n",
       "826  his mate hide his textbooks in the refrigerato...      1           1  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = make_predictions(trainer, test_dataset)\n",
    "test_data[\"prediction\"] = predictions\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gfGlNusLDepj"
   },
   "source": [
    "The **Subtasks B** of **ComVE** is evaluated using accuracy. The [evaluate](https://huggingface.co/docs/evaluate/index) library provides support to apply this and other metrics. The `evaluate_prediction` function takes the test `DataFrame` and calculates the accuracy comparing the `prediction` and `label` columns. With `shrink_dataset` and `base_model` set to `True` the model is not able to learn the task so the expected score is only *0.51*. With a full training run, i.e. with `shrink_dataset` and `base_model` set to `False`, the score should be around *0.928*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Hy6iw9IsDepj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.52}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_prediction(test_data):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy.compute(predictions=test_data[\"prediction\"].values, references=test_data[\"label\"].values)\n",
    "evaluate_prediction(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
