{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zDBF1PXEDfhw"
   },
   "source": [
    "# Pre-trained Language Models: SubTask C \n",
    "## [8 Marks]\n",
    "\n",
    "In this assignment, you will work on the [ComVE](https://competitions.codalab.org/competitions/21080) shared task that was part of SemEval-2020. The task aims to evaluate whether a system can distinguish if a natural language statement makes sense to humans or not and provide a reason. **ConVE** includes three subtasks that require models to acquire and apply commonsense knowledge. In this notebook you will focus on **SubTask C**:\n",
    "\n",
    "- Given a statement that does not make sense, generate the reason why this statement does not make sense. For each nonsensical statement, three valid reasons are given as reference:\n",
    "\n",
    "     *Statement*: He put an elephant into the fridge.  \n",
    "     *Reason A*: An elephant is much bigger than a fridge.  \n",
    "     *Reason B*: A fridge is much smaller than an elephant.  \n",
    "     *Reason C*: Most of the fridges aren't large enough to contain an elephant.\n",
    "\n",
    "     This subtask can be approached as a Sequence-to-Sequence problem where the input is the nonsensical statement and the output is a valid reason.\n",
    "\n",
    "You will fine-tune a Pre-trained Language Model with [Transformers](https://huggingface.co/docs/transformers/index) library that provides a set of tools for fine-tunning and deploying a wide variety of Pre-trained Language Models. The [Hugging Face Hub](https://huggingface.co/models) allows you to explore all the models supported by **Transformers** and even share your own models with the community. In this assignment, you will work with [BART](https://huggingface.co/docs/transformers/model_doc/bart), a pre-trained Sequence-to-Sequence model.\n",
    "\n",
    "Fine-tuning a Pre-trained Language Model usually requires a great amount of time and computational resources. Your personal computer will not be enough. In order to complete the assignment, you can work with a reduced version of the dataset and the base version of **BART**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "X6DavXXpDfhy"
   },
   "outputs": [],
   "source": [
    "shrink_dataset = True\n",
    "base_model = True\n",
    "colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fI5Uu0_gDfhz"
   },
   "source": [
    "Although the value of these variables do not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` and a `base_model` variables set as `True`, and a `colab` variable set as `False`.\n",
    "\n",
    "If you want to perform a full training of the model to obtain its real performance, you can use a cloud service like [Google Colab](https://colab.research.google.com/). **Colab** is a **Jupyter** notebook environment that supports both GPU and TPU instances, allowing training large scale Deep Learning models. Set the `shrink_dataset` and a `base_model` variables to `False`, the `colab` variable to `True`, and follow the instructions provided to you to run the notebook in **Colab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gEqfRl12Dfhz"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    ! pip install transformers datasets evaluate\n",
    "    import os\n",
    "    if not os.path.exists(\"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"):\n",
    "        ! git clone https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.git SemEval2020-Task4-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "U1c3v4XVDfhz"
   },
   "source": [
    "You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Nef4cdirDfhz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer, \n",
    "                          DataCollatorForSeq2Seq, enable_full_determinism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rjm8g6k8Dfhz"
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In Transformers, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9GC6UlXcDfh0"
   },
   "outputs": [],
   "source": [
    "enable_full_determinism(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J8U_rnboDfh0"
   },
   "source": [
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8eBiASC4Dfh0"
   },
   "outputs": [],
   "source": [
    "epochs = 3  # Number of epochs to train the model\n",
    "train_batch_size = 8  # Number of examples used per gradient update\n",
    "learning_rate = 1e-5  # The learning rate for the optimizer\n",
    "max_length = 25  # Maximum lenght of the input sequence\n",
    "output_dir = \"modelC\"  # The output directory where the model will be written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eAd8K6wXDfh0"
   },
   "source": [
    "> **Note!** The notebook for this assignment provides very little guidance. You are expected to refer to the [documentation](https://huggingface.co/docs) for details on how to solve the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kGbzR7H8Dfh0"
   },
   "source": [
    "## Loading the Pre-trained Model - [1 Mark]\n",
    "\n",
    "The first step you must perform in this assignment is to load the model and its corresponding tokenizer using the classes imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "Yk7tOsLaDfh0"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):   #[1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lEVPC3rlDfh0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhay/Desktop/college/uoa/applied_nlp/assignments/assignment_4/env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
      "loading file merges.txt from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/abhay/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\" if base_model else \"facebook/bart-large\"\n",
    "model, tokenizer = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-q7CcBksDfh1"
   },
   "source": [
    "## Data Pre-processing - [2 Marks]\n",
    "\n",
    "The **ComVE** dataset consists of 10000 nonsensical statements for the train set, 997 statements for development and 1000 for test. Each nonsensical statements comes with with three reference valid reasons. You must load the three sets into three `DataFrames`. For the training and development splits, the `DataFrame` should contain three columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and a `reason` column with the reference reasons. For the test set, the `DataFrame` should contain five columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and three columns (`reason1`, `reason2` and `reason3`) containing each of the reference reasons.\n",
    "\n",
    "Train DataFrame:\n",
    "\n",
    "|       |   id | FalseSent                                         | reason                                                                         |\n",
    "|------:|-----:|:--------------------------------------------------|:-------------------------------------------------------------------------------|\n",
    "|   769 |  769 | Computers is an ingredient used in preparing food | Computers are not used for food and they are not edible                        |\n",
    "| 10769 |  769 | Computers is an ingredient used in preparing food | Computer is not something that can be used in preparing food.                  |\n",
    "| 20769 |  769 | Computers is an ingredient used in preparing food | You cannot eat a computer                                                      |\n",
    "|   888 |  888 | he did hear music in his cooling glass            | cooling glass can not play the song, it's not a electronic thing to play music |\n",
    "| 10888 |  888 | he did hear music in his cooling glass            | Glass does not produce music.                                                  |\n",
    "| 20888 |  888 | he did hear music in his cooling glass            | Any sound that might be made by a cooling glass is not music.                  |\n",
    "\n",
    "Test DataFrame:\n",
    "\n",
    "|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            |\n",
    "|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|\n",
    "|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      |\n",
    "| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   |\n",
    "| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. |\n",
    "| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        |\n",
    "| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 |\n",
    "| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "GVeX5et-Dfh1"
   },
   "outputs": [],
   "source": [
    "def load_data(data_csv, answers_csv, is_test=False):   # 1 Mark\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    # Loading the csv files\n",
    "    answers_df = pd.read_csv(answers_csv, header=None).rename(columns={0:'id',1:'reason1',2:'reason2',3:'reason3'})\n",
    "    data_df = pd.read_csv(data_csv)\n",
    "    # Preparing train and development datasets\n",
    "    if not is_test:\n",
    "        expanded_answers_df = answers_df.melt(id_vars=['id'], value_vars=['reason1', 'reason2', 'reason3'], value_name='reason')\n",
    "        data_df = pd.merge(data_df, expanded_answers_df, on='id')\n",
    "        data_df = data_df[['id','FalseSent','reason']]\n",
    "    # preparing test dataset\n",
    "    else:\n",
    "        data_df = pd.merge(data_df,answers_df,on='id')\n",
    "    return data_df\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tE231iQzDfh1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FalseSent</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>769</td>\n",
       "      <td>Computers is an ingredient used in preparing food</td>\n",
       "      <td>Computers are not used for food and they are not edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>769</td>\n",
       "      <td>Computers is an ingredient used in preparing food</td>\n",
       "      <td>Computer is not something that can be used in preparing food.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>769</td>\n",
       "      <td>Computers is an ingredient used in preparing food</td>\n",
       "      <td>You cannot eat a computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>888</td>\n",
       "      <td>he did hear music in his cooling glass</td>\n",
       "      <td>cooling glass can not play the song, it's not a electronic thing to play music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>888</td>\n",
       "      <td>he did hear music in his cooling glass</td>\n",
       "      <td>Glass does not produce music.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>888</td>\n",
       "      <td>he did hear music in his cooling glass</td>\n",
       "      <td>Any sound that might be made by a cooling glass is not music.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          FalseSent  \\\n",
       "2307  769  Computers is an ingredient used in preparing food   \n",
       "2308  769  Computers is an ingredient used in preparing food   \n",
       "2309  769  Computers is an ingredient used in preparing food   \n",
       "2664  888             he did hear music in his cooling glass   \n",
       "2665  888             he did hear music in his cooling glass   \n",
       "2666  888             he did hear music in his cooling glass   \n",
       "\n",
       "                                                                              reason  \n",
       "2307                         Computers are not used for food and they are not edible  \n",
       "2308                   Computer is not something that can be used in preparing food.  \n",
       "2309                                                       You cannot eat a computer  \n",
       "2664  cooling glass can not play the song, it's not a electronic thing to play music  \n",
       "2665                                                   Glass does not produce music.  \n",
       "2666                   Any sound that might be made by a cooling glass is not music.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FalseSent</th>\n",
       "      <th>reason1</th>\n",
       "      <th>reason2</th>\n",
       "      <th>reason3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1280</td>\n",
       "      <td>Beer that is drunk by humans is white</td>\n",
       "      <td>Beer is made of barley and it is a yellow drink</td>\n",
       "      <td>A beer that is drunk by humans is not white.</td>\n",
       "      <td>Beer is brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>860</td>\n",
       "      <td>eating trash food every day makes you stronger</td>\n",
       "      <td>eating trash food every day makes your body fat and weak</td>\n",
       "      <td>eating trash food every day is bad for your health</td>\n",
       "      <td>Trash food could be contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>777</td>\n",
       "      <td>he put some cooking oil in his wine</td>\n",
       "      <td>cooking oil will destroy the taste of the wine</td>\n",
       "      <td>Cooking oil does not go in wine</td>\n",
       "      <td>Cooking oil does not taste nice and therefore would ruin the wine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>570</td>\n",
       "      <td>Lobsters live in the mountains</td>\n",
       "      <td>Lobsters needs water to live</td>\n",
       "      <td>Lobsters live in the sea.</td>\n",
       "      <td>Lobsters live in the sea, not the mountains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1929</td>\n",
       "      <td>the clock shows animals</td>\n",
       "      <td>the clock is used to show the time to people</td>\n",
       "      <td>Clocks are required to tell the time, not show animals</td>\n",
       "      <td>a clock shows the time not animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1619</td>\n",
       "      <td>she put the giraffe in the freezer</td>\n",
       "      <td>A giraffe is much bigger than the freezer</td>\n",
       "      <td>There is no way a giraffe is fitting in the freezer.</td>\n",
       "      <td>A giraffe is too big to be put in a freezer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                       FalseSent  \\\n",
       "76   1280           Beer that is drunk by humans is white   \n",
       "101   860  eating trash food every day makes you stronger   \n",
       "136   777             he put some cooking oil in his wine   \n",
       "174   570                  Lobsters live in the mountains   \n",
       "210  1929                         the clock shows animals   \n",
       "235  1619              she put the giraffe in the freezer   \n",
       "\n",
       "                                                      reason1  \\\n",
       "76            Beer is made of barley and it is a yellow drink   \n",
       "101  eating trash food every day makes your body fat and weak   \n",
       "136            cooking oil will destroy the taste of the wine   \n",
       "174                              Lobsters needs water to live   \n",
       "210              the clock is used to show the time to people   \n",
       "235                 A giraffe is much bigger than the freezer   \n",
       "\n",
       "                                                    reason2  \\\n",
       "76             A beer that is drunk by humans is not white.   \n",
       "101      eating trash food every day is bad for your health   \n",
       "136                         Cooking oil does not go in wine   \n",
       "174                               Lobsters live in the sea.   \n",
       "210  Clocks are required to tell the time, not show animals   \n",
       "235    There is no way a giraffe is fitting in the freezer.   \n",
       "\n",
       "                                                                reason3  \n",
       "76                                                        Beer is brown  \n",
       "101                                    Trash food could be contaminated  \n",
       "136  Cooking oil does not taste nice and therefore would ruin the wine.  \n",
       "174                         Lobsters live in the sea, not the mountains  \n",
       "210                                  a clock shows the time not animals  \n",
       "235                        A giraffe is too big to be put in a freezer.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"\n",
    "train_answers_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_answers_all.csv\"\n",
    "train_data = load_data(train_data_csv, train_answers_csv)\n",
    "dev_data_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_dev_data.csv\"\n",
    "dev_answers_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_gold_answers.csv\"\n",
    "dev_data = load_data(dev_data_csv, dev_answers_csv)\n",
    "test_data_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_test_data.csv\"\n",
    "test_answers_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_gold_answers.csv\"\n",
    "test_data = load_data(test_data_csv, test_answers_csv, True)\n",
    "if shrink_dataset:\n",
    "    idxs = train_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    train_data = train_data[train_data.id.isin(idxs)]\n",
    "    idxs = dev_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    dev_data = dev_data[dev_data.id.isin(idxs)]\n",
    "    idxs = test_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    test_data = test_data[test_data.id.isin(idxs)]\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(\"Train DataFrame:\")\n",
    "display(train_data[:6])\n",
    "print(\"Test DataFrame:\")\n",
    "display(test_data[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DJU-01EtDfh1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 769,\n",
       " 'FalseSent': 'Computers is an ingredient used in preparing food',\n",
       " 'reason': 'Computers are not used for food and they are not edible',\n",
       " '__index_level_0__': 2307}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 1280,\n",
       " 'FalseSent': 'Beer that is drunk by humans is white',\n",
       " 'reason1': 'Beer is made of barley and it is a yellow drink',\n",
       " 'reason2': 'A beer that is drunk by humans is not white.',\n",
       " 'reason3': 'Beer is brown',\n",
       " '__index_level_0__': 76}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "print(\"Train Dataset example:\")\n",
    "display(train_dataset[0])\n",
    "print(\"Test Dataset example:\")\n",
    "display(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HsEcHmGiDfh1"
   },
   "source": [
    "The `Datasets` should be pre-processed following two different approaches. For the test `Dataset`, you must run the tokenizer on the `FalseSent` column and store the result in the `input_ids` and `attention_mask` fields. For the train and development `Datasets` you must also run the tokenizer on the `reason` column and store the resulting `input_ids` in the `labels` field. In all cases, the tokenizer must pad and truncate the sequences to the `max_length` value.\n",
    "\n",
    "><pre>\n",
    ">Train formated Dataset example:\n",
    ">\n",
    ">{'id': 769, 'FalseSent': 'Computers is an ingredient used in preparing food', 'reason': 'Computers are not used for food and they are not edible', '__index_level_0__': 769, 'input_ids': [0, 14721, 43990, 16, 41, 16181, 341, 11, 4568, 689, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 14721, 43990, 32, 45, 341, 13, 689, 8, 51, 32, 45, 27532, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    ">\n",
    ">Test formated Dataset example:\n",
    ">\n",
    ">{'id': 1280, 'FalseSent': 'Beer that is drunk by humans is white', 'reason1': 'Beer is made of barley and it is a yellow drink', 'reason2': 'A beer that is drunk by humans is not white.', 'reason3': 'Beer is brown', '__index_level_0__': 76, 'input_ids': [0, 45562, 14, 16, 10789, 30, 5868, 16, 1104, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "Y17HBOk0Dfh1"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, max_length, is_test=False):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    if not is_test: # performing tokenization on training and development sets\n",
    "        # tokenizing FalseSent column\n",
    "        tokenizer_output = tokenizer(\n",
    "            examples['FalseSent'],padding='max_length',\n",
    "            truncation=True, max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # tokenizing reason column\n",
    "        labels = tokenizer(\n",
    "            examples['reason'],padding='max_length',\n",
    "            truncation=True, max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokenizer_output['labels'] = labels['input_ids']\n",
    "    else: # performing tokenization on test set\n",
    "        tokenizer_output = tokenizer(\n",
    "            examples['FalseSent'], padding='max_length',\n",
    "            truncation=True, max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    return tokenizer_output\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TYDVjG1SDfh1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train formated Dataset example:\n",
      "\n",
      "{'id': 769, 'FalseSent': 'Computers is an ingredient used in preparing food', 'reason': 'Computers are not used for food and they are not edible', '__index_level_0__': 2307, 'input_ids': [0, 14721, 43990, 16, 41, 16181, 341, 11, 4568, 689, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 14721, 43990, 32, 45, 341, 13, 689, 8, 51, 32, 45, 27532, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Test formated Dataset example:\n",
      "\n",
      "{'id': 1280, 'FalseSent': 'Beer that is drunk by humans is white', 'reason1': 'Beer is made of barley and it is a yellow drink', 'reason2': 'A beer that is drunk by humans is not white.', 'reason3': 'Beer is brown', '__index_level_0__': 76, 'input_ids': [0, 45562, 14, 16, 10789, 30, 5868, 16, 1104, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "dev_dataset = dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length, True), batched=True)\n",
    "print(\"Train formated Dataset example:\\n\")\n",
    "print(train_dataset[0])\n",
    "print(\"\\nTest formated Dataset example:\\n\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SE65PJbXDfh2"
   },
   "source": [
    "## Fine-tuning - [5 Marks]\n",
    "\n",
    "In general, when using a `Trainer` to make predictions, it returns the logits for each class in the task. However, the `Seq2SeqTrainingArguments` class provides an option that allows the `Trainer` to generate sequences of tokens in the prediction. The `create_training_arguments` function must create the `Seq2SeqTrainingArguments` with that option and the hyperparamters passed as arguments. During the training, the model must be evaluated on the development set after every epoch. `TrainingArguments` should include this strategy.\n",
    "\n",
    "> **Important!** By default, `Trainer` saves a checkpoint of the model every 500 training steps. For this assignment, avoid this behavior by setting `save_strategy=\"no\"` when creating the `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "NYcGf9LtDfh2"
   },
   "outputs": [],
   "source": [
    "def create_training_arguments(epochs, train_batch_size, learning_rate, output_dir):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        num_train_epochs=epochs, per_device_train_batch_size=train_batch_size,\n",
    "        learning_rate=learning_rate, output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\", save_strategy=\"no\",\n",
    "        predict_with_generate=True\n",
    "    )\n",
    "    return training_args\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OMIWd-JsDfh2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "train_args = create_training_arguments(epochs, train_batch_size, learning_rate, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LlaJ4yJxDfh2"
   },
   "source": [
    "Next, you can create a `Trainer` object initializing the appropriate data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "b3Zqw2vpDfh2"
   },
   "outputs": [],
   "source": [
    "def create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model, args=train_args,\n",
    "        train_dataset=train_dataset, eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer, data_collator=DataCollatorForSeq2Seq(tokenizer)\n",
    "    )\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3E2txek_Dfh2"
   },
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wtQpnSvdDfh2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: FalseSent, id, __index_level_0__, reason. If FalseSent, id, __index_level_0__, reason are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/abhay/Desktop/college/uoa/applied_nlp/assignments/assignment_4/env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 90\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.144694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.438861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.244243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: FalseSent, id, __index_level_0__, reason. If FalseSent, id, __index_level_0__, reason are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: FalseSent, id, __index_level_0__, reason. If FalseSent, id, __index_level_0__, reason are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: FalseSent, id, __index_level_0__, reason. If FalseSent, id, __index_level_0__, reason are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=9.041217380099827, metrics={'train_runtime': 26.7325, 'train_samples_per_second': 10.1, 'train_steps_per_second': 1.347, 'total_flos': 4019258880000.0, 'train_loss': 9.041217380099827, 'epoch': 3.0})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e7Dq4Z3QDfh2"
   },
   "source": [
    "If you have set the `Seq2SeqTrainingArguments` properly, you could now use the `Trainer` to predict sequences of tokens. Take into account that `Trainer` will return the indexes of the tokens, so the sequence must be decoded to obtain the text strings. The `tokenizer` provides functionality to do this. The result of this process can be stored in the `prediction` column of the test `DataFrame`:\n",
    "\n",
    "|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            | prediction                                     |\n",
    "|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------|\n",
    "|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      | Beer that is drunk by humans is white                             |\n",
    "| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   | eating trash food every day makes you stronger |\n",
    "| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. | he put some cooking oil in his wine            |\n",
    "| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        | Lobsters live in mountains                 |\n",
    "| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 | the clock shows animals                        |\n",
    "| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       | she put the giraffe in the freezer             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "8igEq42FDfh2"
   },
   "outputs": [],
   "source": [
    "def make_predictions(trainer, test_dataset, tokenizer):   # [1 Mark]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    test_predictions = trainer.predict(test_dataset=test_dataset)\n",
    "    decoded_predictions = []\n",
    "    for pred in test_predictions.predictions:\n",
    "        decoded_predictions.append(tokenizer.decode(pred,skip_special_tokens=True))\n",
    "    return decoded_predictions\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zaptZPfJDfh2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: reason3, __index_level_0__, FalseSent, id, reason2, reason1. If reason3, __index_level_0__, FalseSent, id, reason2, reason1 are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FalseSent</th>\n",
       "      <th>reason1</th>\n",
       "      <th>reason2</th>\n",
       "      <th>reason3</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1280</td>\n",
       "      <td>Beer that is drunk by humans is white</td>\n",
       "      <td>Beer is made of barley and it is a yellow drink</td>\n",
       "      <td>A beer that is drunk by humans is not white.</td>\n",
       "      <td>Beer is brown</td>\n",
       "      <td>Beer that is drunk by humans is white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>860</td>\n",
       "      <td>eating trash food every day makes you stronger</td>\n",
       "      <td>eating trash food every day makes your body fat and weak</td>\n",
       "      <td>eating trash food every day is bad for your health</td>\n",
       "      <td>Trash food could be contaminated</td>\n",
       "      <td>eating trash food every day makes you stronger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>777</td>\n",
       "      <td>he put some cooking oil in his wine</td>\n",
       "      <td>cooking oil will destroy the taste of the wine</td>\n",
       "      <td>Cooking oil does not go in wine</td>\n",
       "      <td>Cooking oil does not taste nice and therefore would ruin the wine.</td>\n",
       "      <td>he put some cooking oil in his wine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>570</td>\n",
       "      <td>Lobsters live in the mountains</td>\n",
       "      <td>Lobsters needs water to live</td>\n",
       "      <td>Lobsters live in the sea.</td>\n",
       "      <td>Lobsters live in the sea, not the mountains</td>\n",
       "      <td>Lobsters live in the mountains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1929</td>\n",
       "      <td>the clock shows animals</td>\n",
       "      <td>the clock is used to show the time to people</td>\n",
       "      <td>Clocks are required to tell the time, not show animals</td>\n",
       "      <td>a clock shows the time not animals</td>\n",
       "      <td>the clock shows animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1619</td>\n",
       "      <td>she put the giraffe in the freezer</td>\n",
       "      <td>A giraffe is much bigger than the freezer</td>\n",
       "      <td>There is no way a giraffe is fitting in the freezer.</td>\n",
       "      <td>A giraffe is too big to be put in a freezer.</td>\n",
       "      <td>she put the giraffe in the freezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>979</td>\n",
       "      <td>he installed the carpet on the lake</td>\n",
       "      <td>The carpet will absorb water and sink</td>\n",
       "      <td>Carpets need a subfloor.</td>\n",
       "      <td>A lake would not be able to grip onto carpet.</td>\n",
       "      <td>he installed the carpet on the lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>75</td>\n",
       "      <td>My son had us write an essay on The National Monument.</td>\n",
       "      <td>My son isn't smart enough to assign an essay.</td>\n",
       "      <td>My son is studying in the seconds standard only</td>\n",
       "      <td>Children don't ask parents to write essays.</td>\n",
       "      <td>My son had us write an essay on The National Monument.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1810</td>\n",
       "      <td>He drove up the stairs to the bedroom</td>\n",
       "      <td>A car is too large to fit upstairs</td>\n",
       "      <td>Stairs are too small and weak for a car to drive up</td>\n",
       "      <td>People don't drive up indoor stairs.</td>\n",
       "      <td>He drove up the stairs to the bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>774</td>\n",
       "      <td>he put a piece of plastic on the bread</td>\n",
       "      <td>the plastic usually is toxic</td>\n",
       "      <td>You can't eat plastic.</td>\n",
       "      <td>People do not eat plastic because it's not a food</td>\n",
       "      <td>he put a piece of plastic on the bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>1739</td>\n",
       "      <td>My mom always cuts my fish's hair.</td>\n",
       "      <td>Fish don't have any hair.</td>\n",
       "      <td>Fish have scales instead of hair like we have</td>\n",
       "      <td>Fish don't have hair.</td>\n",
       "      <td>My mom always cuts my fish's hair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>324</td>\n",
       "      <td>She put the filing cabinet into the papers.</td>\n",
       "      <td>Nothing can be put into the paper.</td>\n",
       "      <td>it doesnt fit into papers</td>\n",
       "      <td>The filing cabinet is what papers go into.</td>\n",
       "      <td>She put the filing cabinet into the papers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>6</td>\n",
       "      <td>Grizzly bears hate honey.</td>\n",
       "      <td>Honey is good for grizzly bear's growth</td>\n",
       "      <td>Grizzly bears have been observed in the wild seeking out honey to eat</td>\n",
       "      <td>Grizzly bears love honey.</td>\n",
       "      <td>Grizzly bears hate honey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>901</td>\n",
       "      <td>the baby held a grizzly bear</td>\n",
       "      <td>a baby would be eaten by a grizzly bear</td>\n",
       "      <td>Baby can hold a teddy bear but not grizzly bear.</td>\n",
       "      <td>A grizzly bear is bigger than a baby and would eat a baby.</td>\n",
       "      <td>the baby held a grizzly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>343</td>\n",
       "      <td>She throws a stove in the hat.</td>\n",
       "      <td>A stove is bigger than a hat.</td>\n",
       "      <td>stove is too big to fit in a hat</td>\n",
       "      <td>Stove cannot be thrown in the hat</td>\n",
       "      <td>She throws a stove in the hat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1044</td>\n",
       "      <td>fishing nets are useful for covering a window</td>\n",
       "      <td>fishing nets have holes in them that leave gaps of a window uncovered</td>\n",
       "      <td>Fishing net will smell so bad and look so unclean on a window.</td>\n",
       "      <td>Nets don't block out light.</td>\n",
       "      <td>fishing nets are useful for covering a window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>207</td>\n",
       "      <td>Pens are for painting</td>\n",
       "      <td>Pens are a writing utensil</td>\n",
       "      <td>Pens are not brushes.</td>\n",
       "      <td>A pen is not made for painting.</td>\n",
       "      <td>Pens are for painting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>443</td>\n",
       "      <td>My window speaks very well.</td>\n",
       "      <td>Your window cannot speak because it is an object.</td>\n",
       "      <td>windows dont talk</td>\n",
       "      <td>a window does not have a voice</td>\n",
       "      <td>My window speaks very well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1455</td>\n",
       "      <td>The lion used the litter box</td>\n",
       "      <td>A domestic cat is tame and use litter boxes</td>\n",
       "      <td>The lion is too big for a litter box</td>\n",
       "      <td>litter boxes are only for cats</td>\n",
       "      <td>The lion used the litter box</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>13</td>\n",
       "      <td>Cigarette is good for healthy</td>\n",
       "      <td>Lung will be damaged by smoking cigarette</td>\n",
       "      <td>Cigarettes contain carcinogens.</td>\n",
       "      <td>Cigarettes cause cancer.</td>\n",
       "      <td>Cigarette is good for healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>308</td>\n",
       "      <td>Mike ran four sandwiches.</td>\n",
       "      <td>You can't run a sandwich.</td>\n",
       "      <td>A sandwich is not a distance</td>\n",
       "      <td>Sandwich is food and not need to run for it.</td>\n",
       "      <td>Mike ran four sandwiches.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>1439</td>\n",
       "      <td>The cat likes to watch alligators.</td>\n",
       "      <td>Cats and alligators lives in different areas in the world</td>\n",
       "      <td>Cats like to watch things they can eat.</td>\n",
       "      <td>The cat likes to watch birds and not alligators.</td>\n",
       "      <td>The cat likes to watch alligators.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>1696</td>\n",
       "      <td>The supermarket only sells cars</td>\n",
       "      <td>They have a variety of goods and services in the supermarket, mainly household supplies</td>\n",
       "      <td>A supermarket implies the sale of smaller goods like groceries or toiletries.</td>\n",
       "      <td>Supermarkets don't sell cars.</td>\n",
       "      <td>The supermarket only sells cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1548</td>\n",
       "      <td>The man ate the bowl when he was hungry.</td>\n",
       "      <td>People cannot eat bowls.</td>\n",
       "      <td>The bowl is not edible.</td>\n",
       "      <td>One cannot eat a bowl</td>\n",
       "      <td>The man ate the bowl when he was hungry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>863</td>\n",
       "      <td>a flower petal is part of a motor vehicle</td>\n",
       "      <td>a flower petal isn't a motor component</td>\n",
       "      <td>A motor vehicle does not contain flower parts</td>\n",
       "      <td>Petals aren't car parts.</td>\n",
       "      <td>a flower petal is part of a motor vehicle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>745</td>\n",
       "      <td>Renting movies is the newest trend</td>\n",
       "      <td>Fewer and fewer people like renting movies</td>\n",
       "      <td>Renting has been around a long time.</td>\n",
       "      <td>Renting movies is outdated and for something to be trending it must be popular</td>\n",
       "      <td>Renting movies is the newest trend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>1883</td>\n",
       "      <td>People are usually green</td>\n",
       "      <td>No one is born green</td>\n",
       "      <td>People are not green.</td>\n",
       "      <td>No existing human race has green skin.</td>\n",
       "      <td>People are usually green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>1030</td>\n",
       "      <td>beepers are becoming even more popular</td>\n",
       "      <td>no one uses beepers anymore</td>\n",
       "      <td>The beeper trend died in the 90s.</td>\n",
       "      <td>Beepers have gone out of style with all of the new available cell phones, so they are rarely used anymore.</td>\n",
       "      <td>beepers are becoming even more popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>986</td>\n",
       "      <td>the sun rises in the west</td>\n",
       "      <td>the sun rises in the east and sets in the west</td>\n",
       "      <td>It depends on the earth position</td>\n",
       "      <td>The sun always sets in the west</td>\n",
       "      <td>the sun rises in the west</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>905</td>\n",
       "      <td>at night it's easy to find sun</td>\n",
       "      <td>sun can be seen only during the day</td>\n",
       "      <td>By definition night is after the sun has gone</td>\n",
       "      <td>The sun is not out at night</td>\n",
       "      <td>at night it's easy to find sun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               FalseSent  \\\n",
       "76   1280                   Beer that is drunk by humans is white   \n",
       "101   860          eating trash food every day makes you stronger   \n",
       "136   777                     he put some cooking oil in his wine   \n",
       "174   570                          Lobsters live in the mountains   \n",
       "210  1929                                 the clock shows animals   \n",
       "235  1619                      she put the giraffe in the freezer   \n",
       "280   979                     he installed the carpet on the lake   \n",
       "319    75  My son had us write an essay on The National Monument.   \n",
       "371  1810                   He drove up the stairs to the bedroom   \n",
       "411   774                  he put a piece of plastic on the bread   \n",
       "513  1739                      My mom always cuts my fish's hair.   \n",
       "521   324             She put the filing cabinet into the papers.   \n",
       "527     6                               Grizzly bears hate honey.   \n",
       "549   901                            the baby held a grizzly bear   \n",
       "626   343                          She throws a stove in the hat.   \n",
       "636  1044           fishing nets are useful for covering a window   \n",
       "660   207                                   Pens are for painting   \n",
       "678   443                             My window speaks very well.   \n",
       "737  1455                            The lion used the litter box   \n",
       "740    13                           Cigarette is good for healthy   \n",
       "761   308                               Mike ran four sandwiches.   \n",
       "811  1439                      The cat likes to watch alligators.   \n",
       "859  1696                         The supermarket only sells cars   \n",
       "883  1548                The man ate the bowl when he was hungry.   \n",
       "899   863               a flower petal is part of a motor vehicle   \n",
       "902   745                      Renting movies is the newest trend   \n",
       "938  1883                                People are usually green   \n",
       "947  1030                  beepers are becoming even more popular   \n",
       "973   986                               the sun rises in the west   \n",
       "986   905                          at night it's easy to find sun   \n",
       "\n",
       "                                                                                     reason1  \\\n",
       "76                                           Beer is made of barley and it is a yellow drink   \n",
       "101                                 eating trash food every day makes your body fat and weak   \n",
       "136                                           cooking oil will destroy the taste of the wine   \n",
       "174                                                             Lobsters needs water to live   \n",
       "210                                             the clock is used to show the time to people   \n",
       "235                                                A giraffe is much bigger than the freezer   \n",
       "280                                                    The carpet will absorb water and sink   \n",
       "319                                            My son isn't smart enough to assign an essay.   \n",
       "371                                                       A car is too large to fit upstairs   \n",
       "411                                                             the plastic usually is toxic   \n",
       "513                                                                Fish don't have any hair.   \n",
       "521                                                       Nothing can be put into the paper.   \n",
       "527                                                  Honey is good for grizzly bear's growth   \n",
       "549                                                  a baby would be eaten by a grizzly bear   \n",
       "626                                                            A stove is bigger than a hat.   \n",
       "636                    fishing nets have holes in them that leave gaps of a window uncovered   \n",
       "660                                                               Pens are a writing utensil   \n",
       "678                                        Your window cannot speak because it is an object.   \n",
       "737                                              A domestic cat is tame and use litter boxes   \n",
       "740                                                Lung will be damaged by smoking cigarette   \n",
       "761                                                                You can't run a sandwich.   \n",
       "811                                Cats and alligators lives in different areas in the world   \n",
       "859  They have a variety of goods and services in the supermarket, mainly household supplies   \n",
       "883                                                                 People cannot eat bowls.   \n",
       "899                                                   a flower petal isn't a motor component   \n",
       "902                                               Fewer and fewer people like renting movies   \n",
       "938                                                                     No one is born green   \n",
       "947                                                              no one uses beepers anymore   \n",
       "973                                           the sun rises in the east and sets in the west   \n",
       "986                                                      sun can be seen only during the day   \n",
       "\n",
       "                                                                           reason2  \\\n",
       "76                                    A beer that is drunk by humans is not white.   \n",
       "101                             eating trash food every day is bad for your health   \n",
       "136                                                Cooking oil does not go in wine   \n",
       "174                                                      Lobsters live in the sea.   \n",
       "210                         Clocks are required to tell the time, not show animals   \n",
       "235                           There is no way a giraffe is fitting in the freezer.   \n",
       "280                                                       Carpets need a subfloor.   \n",
       "319                                My son is studying in the seconds standard only   \n",
       "371                            Stairs are too small and weak for a car to drive up   \n",
       "411                                                         You can't eat plastic.   \n",
       "513                                  Fish have scales instead of hair like we have   \n",
       "521                                                      it doesnt fit into papers   \n",
       "527          Grizzly bears have been observed in the wild seeking out honey to eat   \n",
       "549                               Baby can hold a teddy bear but not grizzly bear.   \n",
       "626                                               stove is too big to fit in a hat   \n",
       "636                 Fishing net will smell so bad and look so unclean on a window.   \n",
       "660                                                          Pens are not brushes.   \n",
       "678                                                              windows dont talk   \n",
       "737                                           The lion is too big for a litter box   \n",
       "740                                                Cigarettes contain carcinogens.   \n",
       "761                                                   A sandwich is not a distance   \n",
       "811                                        Cats like to watch things they can eat.   \n",
       "859  A supermarket implies the sale of smaller goods like groceries or toiletries.   \n",
       "883                                                        The bowl is not edible.   \n",
       "899                                  A motor vehicle does not contain flower parts   \n",
       "902                                           Renting has been around a long time.   \n",
       "938                                                          People are not green.   \n",
       "947                                              The beeper trend died in the 90s.   \n",
       "973                                               It depends on the earth position   \n",
       "986                                  By definition night is after the sun has gone   \n",
       "\n",
       "                                                                                                        reason3  \\\n",
       "76                                                                                                Beer is brown   \n",
       "101                                                                            Trash food could be contaminated   \n",
       "136                                          Cooking oil does not taste nice and therefore would ruin the wine.   \n",
       "174                                                                 Lobsters live in the sea, not the mountains   \n",
       "210                                                                          a clock shows the time not animals   \n",
       "235                                                                A giraffe is too big to be put in a freezer.   \n",
       "280                                                               A lake would not be able to grip onto carpet.   \n",
       "319                                                                 Children don't ask parents to write essays.   \n",
       "371                                                                        People don't drive up indoor stairs.   \n",
       "411                                                           People do not eat plastic because it's not a food   \n",
       "513                                                                                       Fish don't have hair.   \n",
       "521                                                                  The filing cabinet is what papers go into.   \n",
       "527                                                                                   Grizzly bears love honey.   \n",
       "549                                                  A grizzly bear is bigger than a baby and would eat a baby.   \n",
       "626                                                                           Stove cannot be thrown in the hat   \n",
       "636                                                                                 Nets don't block out light.   \n",
       "660                                                                             A pen is not made for painting.   \n",
       "678                                                                              a window does not have a voice   \n",
       "737                                                                              litter boxes are only for cats   \n",
       "740                                                                                    Cigarettes cause cancer.   \n",
       "761                                                                Sandwich is food and not need to run for it.   \n",
       "811                                                            The cat likes to watch birds and not alligators.   \n",
       "859                                                                               Supermarkets don't sell cars.   \n",
       "883                                                                                       One cannot eat a bowl   \n",
       "899                                                                                    Petals aren't car parts.   \n",
       "902                              Renting movies is outdated and for something to be trending it must be popular   \n",
       "938                                                                      No existing human race has green skin.   \n",
       "947  Beepers have gone out of style with all of the new available cell phones, so they are rarely used anymore.   \n",
       "973                                                                             The sun always sets in the west   \n",
       "986                                                                                 The sun is not out at night   \n",
       "\n",
       "                                                 prediction  \n",
       "76                    Beer that is drunk by humans is white  \n",
       "101          eating trash food every day makes you stronger  \n",
       "136                     he put some cooking oil in his wine  \n",
       "174                          Lobsters live in the mountains  \n",
       "210                                 the clock shows animals  \n",
       "235                      she put the giraffe in the freezer  \n",
       "280                     he installed the carpet on the lake  \n",
       "319  My son had us write an essay on The National Monument.  \n",
       "371                   He drove up the stairs to the bedroom  \n",
       "411                  he put a piece of plastic on the bread  \n",
       "513                      My mom always cuts my fish's hair.  \n",
       "521             She put the filing cabinet into the papers.  \n",
       "527                               Grizzly bears hate honey.  \n",
       "549                                 the baby held a grizzly  \n",
       "626                          She throws a stove in the hat.  \n",
       "636           fishing nets are useful for covering a window  \n",
       "660                                   Pens are for painting  \n",
       "678                             My window speaks very well.  \n",
       "737                            The lion used the litter box  \n",
       "740                           Cigarette is good for healthy  \n",
       "761                               Mike ran four sandwiches.  \n",
       "811                      The cat likes to watch alligators.  \n",
       "859                         The supermarket only sells cars  \n",
       "883                The man ate the bowl when he was hungry.  \n",
       "899               a flower petal is part of a motor vehicle  \n",
       "902                      Renting movies is the newest trend  \n",
       "938                                People are usually green  \n",
       "947                  beepers are becoming even more popular  \n",
       "973                               the sun rises in the west  \n",
       "986                          at night it's easy to find sun  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = make_predictions(trainer, test_dataset, tokenizer)\n",
    "test_data[\"prediction\"] = predictions\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2gAv_hl0Dfh3"
   },
   "source": [
    "The **Subtasks B** of **ComVE** is evaluated using the *bleu* metric. In this assignment, you will also evaluate using *rouge*. With `shrink_dataset` and `base_model` set to `True`, the expected scores are *0.216* and *0.446* for *bleu* and *rouge* respectively. With a full training run, i.e. with `shrink_dataset` and `base_model` set to `False`, the scores should be around *0.228* and *0.461*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "yDJ79cPoDfh3"
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(test_data, metric):   # [2 Marks]\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    metric = evaluate.load(metric)\n",
    "    return metric.compute(predictions=test_data[\"prediction\"].values, references=test_data[[\"reason1\",\"reason2\",\"reason3\"]].values)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MztCvvWyDfh3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.2227191183117467,\n",
       " 'precisions': [0.5756097560975609,\n",
       "  0.30857142857142855,\n",
       "  0.14482758620689656,\n",
       "  0.09565217391304348],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1714285714285715,\n",
       " 'translation_length': 205,\n",
       " 'reference_length': 175}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " evaluate_prediction(test_data, \"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7aIBv1uODfh3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.46834646362123145,\n",
       " 'rouge2': 0.25512620792806556,\n",
       " 'rougeL': 0.44678596257543635,\n",
       " 'rougeLsum': 0.44765685524199467}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_prediction(test_data, \"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "s0D-JhuDDfh3"
   },
   "source": [
    "The scores for the partial training and the full training are so similar that it would appear that the full training does not provide any benefit in this task. However, it should be noted that the test sets in the two cases are different. More importantly, these results are indicative of the limitations of metrics such as *bleu* and *rouge* for evaluating text generation. Take, for example, the following case from the test set:\n",
    "\n",
    "\n",
    "| FalseSent                 | reason1                                        | reason2                          | reason3                         |\n",
    "|:--------------------------|:-----------------------------------------------|:---------------------------------|:--------------------------------|\n",
    "| Beer that is drunk by humans is white | Beer is made of barley and it is a yellow drink | A beer that is drunk by humans is not white. | Beer is brown |\n",
    "\n",
    "The predictions obtained by the partial and full trainings and their corresponding scores are the following:\n",
    "\n",
    "| full training    | prediction                 | bleu     | rouge    |\n",
    "|:-----------------|:---------------------------|---------:|---------:|\n",
    "| no               | Beer that is drunk by humans is white  | 0.731    | 0.889    |\n",
    "| yes              | White beer is not suitable for human consumption. | 0.000    | 0.364    |\n",
    "\n",
    "The text generated by the full training is a better explanation than the reason generated by the partial training, which is a mere repetition of the nonsensical statement. However, the latter obtains much better scores than the former. Metrics such as *bleu* and *rouge* do not always replace accurately the human judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
